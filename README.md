This was my Submission for UCL DDSxFinTech Stock Prediction Hackathon

Training data was supplied in train.csv and the task was to predict the closing price in of the S&P 500 in train.csv. My code is all within main.py.
# S&P 500 Stock Price Prediction - Hackathon Winning Solution ðŸ†

## Overview
This repository contains the source code and data for my winning entry in a competitive Data Science Hackathon (1st Place). 

The objective was to predict the daily closing price of the S&P 500 using 5 years of historical financial data. By implementing a robust feature engineering pipeline and rigorously testing a wide ensemble of regression models, this solution achieved the lowest error rate among all competitors.

## ðŸ“‚ Project Structure

- **`main.py`**: The core executable script. It handles data loading, feature engineering, model training, cross-validation, and generating the final predictions.
- **`train.csv`**: Historical stock data used to train the models (5-year window).
- **`test.csv`**: The holdout dataset used for the final challenge predictions (features only).
- **`prediction.csv`**: The final output file containing the predicted closing prices generated by the model.

## ðŸ§  Technical Approach

### 1. Feature Engineering
The decisive factor in this challenge was not just model complexity, but data representation. The pipeline transforms raw time-series data into:
- **Technical Indicators:** Moving averages, momentum indicators, and volatility metrics.
- **Cyclical Features:** Date-related features (day of week, month, quarter) encoded cyclically to preserve temporal relationships and seasonality.

### 2. Model Selection Strategy
I implemented a comprehensive testing workflow to evaluate performance across a spectrum of linear and tree-based algorithms. The following models were benchmarked:
- **Tree-Based/Ensemble:** `XGBRegressor`, `LGBMRegressor`, `CatBoostRegressor`, `RandomForestRegressor`, `GradientBoostingRegressor`, `HistGradientBoostingRegressor`, `ExtraTreesRegressor`.
- **Linear/Regularized:** `Ridge`, `Lasso`, `ElasticNet`.

### 3. The Winning Model
While gradient boosting methods are popular, my analysis showed that the high noise-to-signal ratio in daily stock data led tree-based models to overfit. 

**Winner:** `Ridge Regression` (alpha = 0.05).
This linear model with L2 regularization provided the best generalization capability, effectively handling multicollinearity among the technical features while remaining robust to market noise.

## ðŸš€ How to Run

1. Clone the repository:

   git clone [https://github.com/fl485/UCL-stock-prediction-hackathon](https://github.com/fl485/UCL-stock-prediction-hackathon)

   
2. Install dependencies (ensure you have pandas, sklearn, numpy, etc.):

  pip install -r requirements.txt

3. Run the main script:

  python main.py


This will process train.csv and test.csv, train the Ridge model, and overwrite prediction.csv with new results.

## ðŸ“Š Results

Rank: 1st / 35 entries.

Key Takeaway: In financial time-series forecasting, rigorous feature engineering and regularization often outperform raw model complexity.


***

### A quick tip for your repo:
Since you mentioned `main.py` is the workflow, make sure you have a `requirements.txt` file in that repo as well (listing pandas, scikit-learn, xgboost, etc.). If you don't have one, you can generate it easily by running `pip freeze > requirements.txt` in your terminal while in your project environment.
